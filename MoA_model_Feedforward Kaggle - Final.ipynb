{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final Kaggle version version.\\\n",
    "This notebook has only the final model. To see why we decided to run this model, please see the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable), and you're done!\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the Data\n",
    "#Load the data. The path is the same for everyone.\n",
    "# train_features_set = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "# train_labels_set = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "# test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "\n",
    "train_features_set = pd.read_csv('train_features.csv') \n",
    "train_labels_set = pd.read_csv('train_targets_scored.csv')\n",
    "test_features = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Lets separate train_features and train_targets_scored into train_set and validation_set.\n",
    "This is important to avoid overfitting.\n",
    "'''\n",
    "\n",
    "#Remove control observations.\n",
    "train_features_without_control = train_features_set[train_features['cp_type'] != 'ctl_vehicle'].copy()\n",
    "train_labels_without_control = train_labels_set.iloc[train_features_without_control.index].copy()\n",
    "\n",
    "#reset index. \n",
    "train_features_without_control.reset_index(inplace = True)\n",
    "train_features_without_control.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "train_labels_without_control.reset_index(inplace = True)\n",
    "train_labels_without_control.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# For the test dataset, we will remove just the column 'cp_type'.\n",
    "# However, we will not delete the observations. \n",
    "# Instead, we will create a separate data frame with 'cp_type' so we can set all MoA equal to zero for these observations.\n",
    "\n",
    "\n",
    "test_features_without_control = test_features.drop(['cp_type'], axis = 1).copy()\n",
    "test_features_control = test_features[['sig_id', 'cp_type']].copy()\n",
    "\n",
    "\n",
    "#These are the same adjustments used before.\n",
    "train_x = train_features_without_control.drop(['sig_id', 'cp_type'], axis = 1).copy()\n",
    "test_x = test_features_without_control.drop(['sig_id'], axis = 1).copy()\n",
    "test_x_control = test_features_control.copy()\n",
    "\n",
    "train_y = train_labels_without_control.drop(['sig_id'], axis = 1).copy()\n",
    "\n",
    "train_x['cp_dose'] = train_x['cp_dose'].str.extract(r\"([1-2])\", expand = True).astype(np.int8)\n",
    "test_x['cp_dose'] = test_x['cp_dose'].str.extract(r\"([1-2])\", expand = True).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some auxiliary function for our model.\n",
    "Important: we are using Adam optimizer, and F.binary_cross_entropy as our loss function.\n",
    "'''\n",
    "def log_loss_mean(y_real, predictions):\n",
    "    y_real_db = pd.DataFrame(data = y_real)\n",
    "    \n",
    "    log_loss_aux = []\n",
    "    for col in y_real_db:\n",
    "        log_loss_aux.append(log_loss(y_real_db[col].astype(float), predictions[col].astype(float), labels = [0, 1]))\n",
    "        \n",
    "    return np.mean(log_loss_aux)\n",
    "\n",
    "'''\n",
    "From: https://github.com/Varal7/ml-tutorial/blob/master/Part2.ipynb\n",
    "In most ML applications we do mini-batch stochastic gradient descent instead of pure stochastic gradient descent.\n",
    "\n",
    "Mini-batch SGD is a step between full gradient descent and stochastic gradient descent by computing the average gradient over a small number of examples.\n",
    "\n",
    "In a nutshell, given n examples:\n",
    "\n",
    "Full GD: dL/dw = average over all n examples. One step per n examples.\n",
    "SGD: dL/dw = point estimate over a single example. n steps per n examples.\n",
    "Mini-batch SGD: dL/dw = average over m << n examples. n / m steps per n examples.\n",
    "Advantages of mini-batch SGD include a more stable gradient estimate and computational efficiency on modern hardware (exploiting parallelism gives sub-linear to constant time complexity, especially on GPU).\n",
    "\n",
    "In PyTorch, batched tensors are represented as just another dimension. Most of the deep learning modules assume batched tensors as input (even if the batch size is just 1).\n",
    "\n",
    "Code from: MITx: 6.86x (Project 3)\n",
    "'''\n",
    "def batchify_data(x_data, y_data, batch_size):\n",
    "    \"\"\"Takes a set of data points and labels and groups them into batches.\"\"\"\n",
    "    # Only take batch_size chunks (i.e. drop the remainder)\n",
    "    N = int(len(x_data) / batch_size) * batch_size\n",
    "    batches = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        batches.append({\n",
    "            'x': torch.tensor(x_data[i:i+batch_size], dtype=torch.float32),\n",
    "            'y': torch.tensor(y_data[i:i+batch_size], dtype=torch.long\n",
    "        )})\n",
    "    return batches\n",
    "\n",
    "def compute_accuracy(predictions, y):\n",
    "    \"\"\"Computes the accuracy of predictions against the gold labels, y.\"\"\"\n",
    "    predictions_np = predictions.detach().numpy()\n",
    "    y_np = y.detach().numpy()\n",
    "    return np.mean(np.equal(predictions_np, y_np))\n",
    "\n",
    "\n",
    "# Training Procedure\n",
    "def train_model_SGD(train_data, dev_data, model, lr=0.01, momentum=0.9, nesterov = False, n_epochs=30, save_model = 'FNN_SGD'):  # If we use SGD.\n",
    "    \"\"\"Train a model for N epochs given data and hyper-params.\"\"\"\n",
    "    # We optimize with SGD\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, nesterov=nesterov)\n",
    "\n",
    "    for epoch in range(1, n_epochs):\n",
    "        print(\"-------------\\nEpoch {}:\\n\".format(epoch))\n",
    "\n",
    "\n",
    "        # Run **training***\n",
    "        train_loss, train_acc, train_out = run_epoch(train_data, model.train(), optimizer)\n",
    "        print('Train loss: {:.6f} | Train accuracy: {:.6f}'.format(train_loss, train_acc))\n",
    "\n",
    "        # Run **validation**\n",
    "        dev_loss, dev_acc, dev_out = run_epoch(dev_data, model.eval(), optimizer)\n",
    "        print('Val loss:   {:.6f} | Val accuracy:   {:.6f}'.format(dev_loss, dev_acc))\n",
    "        # Save model\n",
    "        torch.save(model, save_model + '.pt')\n",
    "    return \n",
    "\n",
    "\n",
    "def train_model_Adam(train_data, dev_data, model, lr = 0.01, weight_decay = 1e-5, n_epochs=30, save_model = 'FNN_Adam'):  # If we use Adam.\n",
    "    \"\"\"Train a model for N epochs given data and hyper-params.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "\n",
    "    for epoch in range(1, n_epochs):\n",
    "        print(\"-------------\\nEpoch {}:\\n\".format(epoch))\n",
    "\n",
    "\n",
    "        # Run **training***\n",
    "        train_loss, train_acc, train_out = run_epoch(train_data, model.train(), optimizer)\n",
    "        print('Train loss: {:.6f} | Train accuracy: {:.6f}'.format(train_loss, train_acc))\n",
    "\n",
    "        # Run **validation**\n",
    "        dev_loss, dev_acc, dev_out = run_epoch(dev_data, model.eval(), optimizer)\n",
    "        print('Val loss:   {:.6f} | Val accuracy:   {:.6f}'.format(dev_loss, dev_acc))\n",
    "        # Save model\n",
    "        torch.save(model, save_model + '.pt')\n",
    "    return \n",
    "\n",
    "def run_epoch(data, model, optimizer):\n",
    "    \"\"\"Train model for one pass of train data, and return loss, acccuracy\"\"\"\n",
    "    # Gather losses\n",
    "    losses = []\n",
    "    batch_accuracies = []\n",
    "\n",
    "    # If model is in train mode, use optimizer.\n",
    "    is_training = model.training\n",
    "\n",
    "    # Iterate through batches\n",
    "    for batch in tqdm(data):\n",
    "        # Grab x and y\n",
    "        x, y = batch['x'], batch['y']\n",
    "       \n",
    "        # Get output predictions\n",
    "        out = model(x)\n",
    "        y = y.type_as(out)\n",
    "        \n",
    "        \n",
    "        # Predict and store accuracy      \n",
    "        predictions = torch.round(out)\n",
    "        batch_accuracies.append(compute_accuracy(predictions, y))\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.binary_cross_entropy(out, y)\n",
    "        losses.append(loss.data.item())\n",
    "\n",
    "        # If training, do an update.\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Calculate epoch level scores\n",
    "    avg_loss = np.mean(losses)\n",
    "    avg_accuracy = np.mean(batch_accuracies)\n",
    "    return avg_loss, avg_accuracy, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model 1 hidden layer SGD\n",
    "start = time.time()\n",
    "\n",
    "X_train = train_x.to_numpy(copy = True)\n",
    "X_dev = test_x.to_numpy(copy = True)\n",
    "\n",
    "Y_train = train_y.to_numpy(copy = True)\n",
    "Y_dev = test_y.to_numpy(copy = True)\n",
    "\n",
    "n_epochs = 290               #Hyperparameter\n",
    "batch_size = 32             #Hyperparameter\n",
    " \n",
    "train_batches = batchify_data(X_train, Y_train, batch_size)\n",
    "dev_batches = batchify_data(X_dev, Y_dev, batch_size)\n",
    "\n",
    "train_batches_full_base = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "#################################\n",
    "## Model specification\n",
    "model = nn.Sequential(\n",
    "                    nn.Linear(X_train.shape[1], int(0.5*X_train.shape[1])), \n",
    "                    nn.ReLU6(),  \n",
    "                    nn.Linear(int(0.5*X_train.shape[1]), 206),  \n",
    "                    nn.Sigmoid(),  #So we have probabilities at the end\n",
    "                )\n",
    "lr = 0.1 # Hyperparameter\n",
    "momentum = 0\n",
    "##################################\n",
    "\n",
    "#Train model\n",
    "train_model_SGD(train_batches, model, lr = lr, momentum = momentum, nesterov = False, n_epochs = n_epochs, save_model = 'FNN_layer_1')\n",
    "\n",
    "# Get output.\n",
    "# Evaluate the model on test data.\n",
    "out_train = model(train_batches_full_base) \n",
    "out_dev = model(dev_batches_full_base)    \n",
    "\n",
    "out_train = out_train.detach().numpy()\n",
    "out_train = pd.DataFrame(data = out_train)\n",
    "\n",
    "# Adjust out_dev for the control group\n",
    "out_dev = out_dev.detach().numpy()\n",
    "# Make a matrix with zeros when control.\n",
    "treatment_flag = test_x_control['cp_type'] == 'trt_cp'\n",
    "treatment_flag = np.array([treatment_flag])\n",
    "treatment_flag = np.repeat(treatment_flag.transpose(), 206, axis = 1)\n",
    "out_dev = out_dev*treatment_flag\n",
    "out_dev = pd.DataFrame(data = out_dev)\n",
    "    \n",
    "print()\n",
    "print('Log loss train:', log_loss_mean(Y_train, out_train))\n",
    "\n",
    "#Add columns name\n",
    "out_dev.columns = train_y.columns\n",
    "#Add column 'sig_id'\n",
    "out_dev = test_x_control[['sig_id']].join(out_dev)\n",
    "#Submit\n",
    "# out_dev.to_csv('submission_SGD.csv', index = False) \n",
    "out_dev.to_csv('submission.csv', index = False) \n",
    "\n",
    "# Log loss train: 0.013825678791504065\n",
    "# Log loss dev: 0.01545215945048286    (Considering 20% of the train database)\n",
    "# Log loss LB: 0.01972\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print()\n",
    "print('elapsed time:',elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
